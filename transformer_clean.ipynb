{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# P2020 Town Hall\n",
        "## Google, 2024-09-17\n",
        "\n",
        "![Machine Learning](https://imgs.xkcd.com/comics/machine_learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-VIf5URNM7q"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Most of this is \"borrowed\" from a bunch of examples. See [Sources](#Sources).\n",
        "\n",
        "To open this in Google Colab, click [here](https://colab.research.google.com/github/klao/town-hall-2024/blob/master/transformer_clean.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isiMrnRaOSik",
        "outputId": "d925b5c0-2863-45ca-a0a5-16e8eefae8c4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    %pip install einops\n",
        "    %pip install jaxtyping\n",
        "    %pip install transformer_lens\n",
        "    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "else:\n",
        "    # See README.md for local setup\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHL1yEOAPu3v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import plotly.express as px\n",
        "import torch as t\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "from jaxtyping import Int, Float\n",
        "from typing import List, Optional, Tuple\n",
        "import functools\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import webbrowser\n",
        "import gdown\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "from transformer_lens.utils import get_corner\n",
        "import circuitsvis as cv\n",
        "\n",
        "# Saves computation time, since we don't need it for the contents of this notebook\n",
        "t.set_grad_enabled(False)\n",
        "\n",
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTw0C8JsQNzi"
      },
      "source": [
        "# Let's look at the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAjxyDmVCVbh",
        "outputId": "6e8de37d-34b3-4ff3-90c6-27063f4a6fa8"
      },
      "outputs": [],
      "source": [
        "gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt2.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usYlUFIoQt0T"
      },
      "source": [
        "## Input: \"What does this eat?\" aka Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt2.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_sorted = sorted(gpt2.tokenizer.vocab.items(), key=lambda x: x[1])\n",
        "vocab_sorted[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_sorted[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Hullo, my name is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(gpt2.to_str_tokens(text))\n",
        "print(gpt2.to_tokens(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output: \"What comes out?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = gpt2(gpt2.to_tokens(text)).squeeze()\n",
        "predictions = predictions.softmax(-1)\n",
        "print(predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_predictions = predictions[-1, :].topk(5)\n",
        "best_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in best_predictions.indices:\n",
        "    print(f'\"{gpt2.tokenizer.decode([k])}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Longer example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = (\"This is a story about Quomatarus.\"\n",
        "  + \" When one day Quomatarus decided\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = gpt2.to_tokens(text)\n",
        "str_tokens = gpt2.to_str_tokens(text)\n",
        "print(str_tokens)\n",
        "print(tokens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUrk3ocP_Oit",
        "outputId": "87d79e03-9ca3-4dd3-ab96-ea71c43d58d8"
      },
      "outputs": [],
      "source": [
        "logits, cache = gpt2.run_with_cache(tokens, remove_batch_dim=True)\n",
        "print(logits.shape)\n",
        "print(cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logits to probabilities\n",
        "\n",
        "probs = logits.softmax(-1).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How well did it predict the actual tokens?\n",
        "x = probs.squeeze()[:, tokens.squeeze()]\n",
        "predictions = x.diag(1)\n",
        "t.round(predictions, decimals=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot it! Which tokens did it do well on? Which poorly? Why?\n",
        "px.line(predictions.cpu(), hover_name=str_tokens[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cache[\"pattern\", 0, \"attn\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at attention patterns\n",
        "# cv.attention.attention_pattern(s)\n",
        "\n",
        "# Compare block 0 head 5 to block 5 head 5!\n",
        "cv.attention.attention_pattern(cache[\"pattern\", 0, \"attn\"][5, :, :], tokens=str_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "attention_pattern = cache[\"pattern\", 5, \"attn\"].squeeze()\n",
        "\n",
        "display(cv.attention.attention_patterns(\n",
        "    tokens=str_tokens,\n",
        "    attention=attention_pattern,\n",
        "    attention_head_names=[f\"H{i}\" for i in range(12)],\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coJLiIBg4FzK"
      },
      "source": [
        "# Sources\n",
        "\n",
        "These are the main inspirations:\n",
        "\n",
        "* https://arena-ch1-transformers.streamlit.app/[1.2]_Intro_to_Mech_Interp\n",
        "* https://transformer-circuits.pub/2021/framework/index.html\n",
        "\n",
        "Videos:\n",
        "\n",
        "* https://neelnanda.io/transformer-tutorial"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
